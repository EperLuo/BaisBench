{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the script for task2 of AISBench. This script includes downloading the dataset, obtaining the background information, and getting the multiple-choice question and their reference answers. You can fed these to the AI scientists and ask them to answer the question based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_id = 2\n",
    "\n",
    "df = pd.read_excel('Task2_data/BAISBench_task2.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "question_name = df['name'][question_id]\n",
    "background_info = df[df['name']==question_name]['background'].values.item()\n",
    "\n",
    "question_list = []\n",
    "question_answer = []\n",
    "for i in range(1,6):\n",
    "    question_list.append(df[df['name']==question_name][f'Questions{i}'].values.item())\n",
    "    answers = re.findall(r'\\b([A-Z])\\)', df[df['name']==question_name][f'Answer{i}'].values.item())\n",
    "    question_answer.append(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "\n",
    "# 设置 repo 名称\n",
    "repo_id = \"EperLuo/BAISBench\" \n",
    "repo_type = \"dataset\" \n",
    "\n",
    "# 设置前缀\n",
    "prefix = f\"task2 - {question_name}\"\n",
    "\n",
    "# 列出所有文件\n",
    "all_files = list_repo_files(repo_id=repo_id, repo_type=repo_type)\n",
    "\n",
    "# 筛选出带有指定前缀的文件\n",
    "target_files = [f for f in all_files if f.startswith(prefix)]\n",
    "\n",
    "# 下载这些文件\n",
    "for file_name in target_files:\n",
    "    local_path = hf_hub_download(\n",
    "        repo_id=repo_id, \n",
    "        filename=file_name, \n",
    "        repo_type=repo_type,\n",
    "        local_dir=\"Task2_data\", \n",
    "        local_dir_use_symlinks=False )\n",
    "    print(f\"Downloaded: {file_name} -> {local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then feed these questions into AI scientist to analysis the data and obtain the answer. Below are the AI scientist we evaluated in our benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomni\n",
    "To run Biomni in the task2, you need to first install the environment for Biomni. Please refer to https://github.com/snap-stanford/biomni for the details.\n",
    "\n",
    "After set the api key and base url, run:\n",
    "```\n",
    "cd model_zoo/Biomni\n",
    "python run_task2_claude_sonnet.py\n",
    "```\n",
    "This will run all questions in task2 iteratively with Claude sonnet model. You can modifiy the model and base url in this python file. The results will be saved at ./output_claude_sonnet.\n",
    "\n",
    "Below is the script of results evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quest = pd.read_excel(\"Task2_data/BAISBench_task2.xlsx\", sheet_name='Sheet1')\n",
    "all_cato = np.load('Task2_data/BAISBench_task2_categories.npy', allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions and answers\n",
    "question_list = []\n",
    "question_answer = []\n",
    "# Read question and background information\n",
    "for question_id in range(41):\n",
    "    question_name = df_quest['name'][question_id]\n",
    "    background_info = df_quest[df_quest['name']==question_name]['background'].values.item()\n",
    "\n",
    "    for i in range(1,6):\n",
    "        if pd.isna(df_quest[df_quest['name']==question_name][f'Questions{i}'].values.item()):\n",
    "            continue\n",
    "        else:\n",
    "            question_list.append(df_quest[df_quest['name']==question_name][f'Questions{i}'].values.item())\n",
    "            answers = re.findall(r'\\b([A-Z])\\)', df_quest[df_quest['name']==question_name][f'Answer{i}'].values.item())\n",
    "            question_answer.append([','.join(answers)][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Analysis of cellular components', 'Cell heterogeneity analysis',\n",
       "        'Cell-cell communication', 'Cellular function reasoning',\n",
       "        'Developmental state analysis', 'Disease analysis',\n",
       "        'Key gene analysis', 'Other', 'Pathway analysis',\n",
       "        'Reasoning & analysis based on data'], dtype='<U34'),\n",
       " array([27, 22,  4, 26, 15, 34, 33,  4,  5, 23]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_cato, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answers(gt_list, pred_list):\n",
    "    \"\"\"\n",
    "    gt_list   : standard answers list ['B', 'A,C,D']\n",
    "    pred_list : AI scientist answers list ['B', 'A,C']\n",
    "    return    : (total_score, score_list)\n",
    "    \"\"\"\n",
    "    assert len(gt_list) == len(pred_list), \"the length of gt_list and pred_list must be the same.\"\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for gt, pred in zip(gt_list, pred_list):\n",
    "        gt_set = set(gt.split(\",\"))\n",
    "        pred_set = set(pred.split(\",\"))\n",
    "\n",
    "        # 单选题\n",
    "        if len(gt_set) == 1:\n",
    "            score = 1.0 if pred_set == gt_set else 0.0\n",
    "\n",
    "        # 多选题\n",
    "        else:\n",
    "            if pred_set == gt_set:\n",
    "                score = 1.0                      # all correct\n",
    "            elif pred_set < gt_set:\n",
    "                score = 0.5                      # missing some correct options\n",
    "            else:\n",
    "                score = 0.0                      # wrong options included\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return sum(scores), scores\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def accuracy_by_type(gt_list, pred_list, type_list):\n",
    "    assert len(gt_list) == len(pred_list) == len(type_list)\n",
    "\n",
    "    total_scores, scores = score_answers(gt_list, pred_list)\n",
    "\n",
    "    stats = defaultdict(lambda: {\n",
    "        \"count\": 0,\n",
    "        \"strict_correct\": 0,\n",
    "        \"total_score\": 0.0\n",
    "    })\n",
    "\n",
    "    for s, t in zip(scores, type_list):\n",
    "        stats[t][\"count\"] += 1\n",
    "        stats[t][\"total_score\"] += s\n",
    "        if s == 1.0:\n",
    "            stats[t][\"strict_correct\"] += 1\n",
    "\n",
    "    # 计算概率\n",
    "    result = {}\n",
    "    for t, v in stats.items():\n",
    "        result[t] = {\n",
    "            \"num_questions\": v[\"count\"],\n",
    "            \"strict_accuracy\": v[\"strict_correct\"] / v[\"count\"],\n",
    "            \"expected_score\": v[\"total_score\"] / v[\"count\"]\n",
    "        }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_biomni_claude = 'B B B D B C C C D A B A B A,C,D B,D A B B,C C B C B B C B A A B C A C C B D A D D B B B B A A D B A C B C C D D C D A,B,C,D A,D A,C A B B D D A A B B B A,B,C A C A B D A C A D B B A A A A A B C A A,B,D,E A A A,C C B A A B A B B A B A B A D D D A A C D D A A A A B C B A C C A,C,D A A B C A A,D A B B B A D C A A A A D B B D A,B,C,D C C B D A D B B A C A B B B C C D B C D C B C C A B B A A C B B B C C C D B A B C B A A B B B A'\n",
    "answer_biomni_claude = answer_biomni_claude.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score: 136.0\n",
      "100 point scale: 70.46632124352331\n",
      "Analysis of cellular components 68.51851851851852\n",
      "Key gene analysis 62.121212121212125\n",
      "Disease analysis 76.47058823529412\n",
      "Cell heterogeneity analysis 72.72727272727273\n",
      "Reasoning & analysis based on data 76.08695652173914\n",
      "Pathway analysis 50.0\n",
      "Developmental state analysis 73.33333333333333\n",
      "Cellular function reasoning 76.92307692307693\n",
      "Cell-cell communication 75.0\n",
      "Other 25.0\n"
     ]
    }
   ],
   "source": [
    "total_scores, score_list =score_answers(question_answer, answer_biomni_claude)\n",
    "print(\"Total Score:\", total_scores)\n",
    "print(\"100 point scale:\", total_scores / len(question_answer) * 100)\n",
    "\n",
    "result = accuracy_by_type(question_answer, answer_biomni_claude, all_cato)\n",
    "\n",
    "for k, v in result.items():\n",
    "    print(k, v['expected_score']*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pantheon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set the environment for Pantheon. Please refer to https://pantheonos.stanford.edu/cli/docs/ or https://github.com/aristoteleo/pantheon-cli for the details.\n",
    "\n",
    "After set the model, api key and base url, run:\n",
    "```\n",
    "cd model_zoo/Pantheon\n",
    "chmod -x run_task2_claude_sonnet.sh\n",
    "bash run_task2_claude_sonnet.sh\n",
    "```\n",
    "\n",
    "The results and analysis process will be saved in model_zoo/Pantheon/output_task2_claude_sonnet. The prompts for this task can be found at model_zoo/Pantheon/prompt_task2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STELLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set the environment for STELLA. Please refer to https://github.com/zaixizhang/STELLA for the details.\n",
    "\n",
    "\n",
    "After set the api key and base url, run:\n",
    "```\n",
    "cd model_zoo/STELLA\n",
    "python stella_core.py --use_template --use_mem0\n",
    "```\n",
    "\n",
    "The prompt for stella can be found in model_zoo/STELLA/prompt_task2\n",
    "\n",
    "The results will be saved at model_zoo/STELLA/output_task2. The analysis can be accessed through the web browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomni_e1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
